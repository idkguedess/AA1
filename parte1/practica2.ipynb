{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2: Preprocesado básico de datos\n",
    "\n",
    "## Aprendizaje Automático I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchos problemas de Aprendizaje Automático es necesario realizar un procesado de los datos antes de aplicar los métodos de aprendizaje. Esta fase se denomina como Ingeniería de Datos y es en si misma una disciplina. Aunque será cubierta con profundidad en la asignatura Análisis Exploratorio de Datos y Visualización, hay procesos muy sencillos como dividir los datos en un conjunto de entrenamiento y otro para prueba, convertir un dato simbólico en uno numérico, o escalar los valores numéricos de un conjunto de datos a un determinado rango. Estas serán las tareas que se realizarán en esta práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores perdidos\n",
    "\n",
    "En determinados problems puede ocurrir que no se disponga de todos los datos porque algunos no se hayan podido obtener, dando lugar a lo que se conoce como valores perdidos. Existen diferentes técnicas para tratarlos pero si el número no es muy alto comparado, una solución muy sencilla es eliminarlos. Hay otras técnicas denominadas de imputación cuyo objetivo es intentar estimar el valor perdido pero esta fuera del ámbito de esta asignatura.\n",
    "\n",
    "Pandas permite la detección de los valores perdidos de forma muy sencilla ya que se detectan con la función isna(). Devuelve un dataframe con las posiciones perdidas a True y el resto a False. En el caso de pasarle una columna, devuelve una serie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicar que la variable columns de un dataframe devuelve el nombre de todas las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris-small-perdidos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devuelve un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perdidos = df.isna()\n",
    "print(perdidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devuelve una serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perdidos = df['sepal length'].isna()\n",
    "print(perdidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio propuesto para hacer en clase\n",
    "\n",
    "Contar el número de valores perdidos para cada una de las variables\n",
    "\n",
    "Posible solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df.columns\n",
    "for variable in variables:\n",
    "    perdidos = df[variable].isna()\n",
    "    print(f'Valores perdidos en variable {variable}: {np.sum(perdidos)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de rangos\n",
    "\n",
    "En algunos casos los datos poseen rangos muy diferentes para las diferentes variables que se consideran y para determinados modelos esto puede dar lugar a un rendimiento muy bajo de los mismos. Por ejemplo los modelos basados en distancias como el vecino más próximo o el centroide más próximo se ven muy afectados si todas las variables no tienen un rango similar. Otro caso son la redes neuronales o las máquinas de vectores soporte.\n",
    "\n",
    "La librería sklearn dispone de un modulo de preprocesado que incluye la normalización de los rangos.\n",
    "\n",
    "Explicar que la función describe muestra algunos estadísticos básicos de las variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris-small.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy posee método para obtener el máximo y mínimo de un array de forma global si no se indica el eje, por columnas si el eje es el 0 o po r filas si el eje es el 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio propuesto para hacer en clase\n",
    "\n",
    "Implementar una función que acepte un array unidimensional (vector) numpy y escale los valores entre 0 y 1. Utilizar las funciones max y min de numpy.\n",
    "\n",
    "Posible solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza(vector):\n",
    "    maximo = np.max(vector)\n",
    "    minimo = np.min(vector)\n",
    "\n",
    "    resultado = (vector - minimo) / (maximo - minimo)\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.randint(20, size=10)\n",
    "print(v)\n",
    "res = normaliza(v)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn incluye en el módulo preprocessing la clase MinMaxScaler que realiza un escalado lineal de las variables entre un valor mínimo y máximo. Por defecto esos valores están entre 0 y 1. La clase devuelve un array numpy aunque puede aceptar dataframes de pandas.\n",
    "\n",
    "La API de sklearn para obtener los parámetros de la mayoría de los modelos y métodos que incluye se basa en llamar al método fit con el que se ajustan los parámetros del modelo. Luego mediante el método transform se aplica el modelo. Si se desea realizar los dos pasos a la vez se puede utilizar el método fit_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = df.values\n",
    "variables = datos[:,:-1]\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler()\n",
    "normalizador.fit(variables)\n",
    "res = normalizador.transform(variables)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(res,axis=0))\n",
    "print(np.min(res,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiar los límites para escalar y utilizar fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler((-2,1))\n",
    "res = normalizador.fit_transform(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(res,axis=0))\n",
    "print(np.amin(res,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir entre train y test\n",
    "\n",
    "Los modelos una vez entrenados y puestos en funcionamiento van a utilizar como entrada, información que no se utilizó en el proceso de entrenamiento. Sin embargo es necesario tener una aproximación al rendimiento del método una vez entrenado y antes de ser puesto en producción. Para simular esta situación, el conjunto de datos disponible se divide en dos subconjuntos. Una parte se utiliza para entrenar el modelo y otra para estimar cómo sería el rendimiento en caso de ser puesto en producción, ya que estos últimos datos no se utilzaron en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir el conjunto iris en 40 para entrenamiento y 20 para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de muestras de iris\n",
    "print(f'Número de muestras: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[:40,:]\n",
    "test = df.iloc[40:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de train no tiene iris-virginica y el de test no tiene iris-setosa ni iris-versicolor por lo tanto el modelo no es capaz de aprender todas las clases.\n",
    "\n",
    "La solución es mezclar aleatoriamente las muestras y luego tomar las 100 y las 50. \n",
    "\n",
    "Existen diferentes opciones para hacerlo. Si se tiene un dataframe de pandas, se dispone de la función sample que devuelve una muestra aleatoria del dataframe indicando que número de elementos o que fracción de los mismos se desea extraer. Si la fracción es 1, se obtiene el mismo dataframe reordenado aleatoriamente.\n",
    "\n",
    "Importante si se van a hacer dos ejecuciones para comparar resultado, poner el valor de rand_state a uno fijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.sample(frac=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería sklearn dispone del módulo model_selection que implementa la función train_test_split que realiza permite dividir un conjunto de datos en dos (train y test) indicando la proporción o número de muestras de cada uno. Además incluye otros argumentos como la semilla del generador de números aleatorios o la opción de estratificar (mantener la proporción de muestras por cada clase) el resultado. También permite indicar si se mezclan antes de la división o no (shuffle)\n",
    "\n",
    "Esta función puede aceptar tanto un dataframe o un array de numpy. También permite pasarle varios argumentos, haciendo la misma división en todos.\n",
    "\n",
    "Explicar el uso de random_state para generar siempre la misma partición. Muy importante para comparar resultados de diferentes modelos para no se vean afectados por las muestras de las particiones y todos los modelos para comparar utilicen siempre el mismo conjunto de datos tanto para entrenar como testear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(20, size=10)\n",
    "y = np.random.randint(200, 250, size=10)\n",
    "z = np.random.random(size=10)\n",
    "\n",
    "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(x, y, z, test_size=0.2)\n",
    "print(x_train)\n",
    "print(x_test)\n",
    "\n",
    "print(z_train)\n",
    "print(z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probar a ejecutar varias veces sin fijar el valor de random_state y fijándolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear conjunto de datos muy desbalanceado para explicar la estratificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(100,size=(100,4))\n",
    "y = np.concatenate([np.zeros(90), np.ones(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin estratificación la muestra minoritaria puede que no esté bien representada en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, stratify=y)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir de valores simbólicos a numéricos (**Solo si hay tiempo**)\n",
    "\n",
    "Algunos modelos no admiten valores simbólicos y por tanto se deben convertir en numéricos. Una posibilidad es asignar un entero a cada valor simbólico existente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La aproximación más sencilla es utilizar un diccionario de Python para realizar la conversión.\n",
    "\n",
    "### Dejar como ejercicio de clase\n",
    "\n",
    "Posible solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "for i in range(len(df)):\n",
    "    df.loc[i,'class'] = dic[df.iloc[i]['class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando el número de valores diferentes es muy grande puede ser un poco engorroso crear el diccionario de forma manual. Por eso sklearn posee una clase que realiza la función de conversión. La clase es LabelEncoder y la interfaz es similar a la de MinMaxScaler con funciones, fit, transform y fit_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris-small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversor = LabelEncoder()\n",
    "conversor.fit(df['class'])\n",
    "res = conversor.transform(df['class'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
