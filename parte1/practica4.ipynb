{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 4. Regresión Lineal (II) - Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso según el gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn implementa el descenso según el gradiente la clase SGDRegressor. Se le debe indicar que función minimizar, en nuestro caso el error cuadrático (loss=squared_error), si se incluye regularización (penalty=None), el learning rate que su valor se indica en eta0 y puede ser constante o variable, en nuestro caso constante (learning_rate='constant'), max_iter es el número máximo de iteraciones. \n",
    "\n",
    "Si el aprendizaje se realiza con el método fit, se realizan las iteraciones hasta converger. Sin embargo, se puede hacer paso a paso mediante partial_fit, de esta forma cada ejecución realiza una única época. Explicar que época es aplicar la corrección con todas las muestras del conjunto de aprendizaje.\n",
    "\n",
    "La clase posee el método score que devuelve el coeficiente de determinación $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('tiempos-minutos.csv', sep=';')\n",
    "# datos = pd.read_csv('datos-hormigon.csv', sep=';', decimal=',')\n",
    "\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobar si existen valores perdidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar ejemplo utilizando fit. Configurar con learning_rate constante, sin regularización. IMPORTANTE: comprobar el resultado sin escalar y escalando los valores. Explicar porque sin escalar se obtienen valores tan altos.\n",
    "\n",
    "Probar con valores diferentes de verbose y varios valores de max_iter para comprobar el warning que da si se ponen pocas iteraciones. P.e. max_iter=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SGDRegressor(loss='squared_error', penalty=None, max_iter=5, learning_rate='constant', eta0=0.001, verbose=0)\n",
    "\n",
    "X = datos.iloc[:,:-1].values\n",
    "y = datos.iloc[:,-1].values\n",
    "\n",
    "regr.fit(X,y)\n",
    "r2 = regr.score(X,y)\n",
    "print(f'R2: {r2}')\n",
    "print(f'N. iteraciones: {regr.n_iter_}')\n",
    "# print(f'Coeficientes: {regr.coef_}')\n",
    "y_pred = regr.predict(X)\n",
    "print(f'score: {r2_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados con escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SGDRegressor(loss='squared_error', penalty=None, max_iter=100, learning_rate='constant', eta0=0.001, verbose=0)\n",
    "\n",
    "X = datos.iloc[:,:-1].values\n",
    "y = datos.iloc[:,-1].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y.reshape(-1,1))\n",
    "y = y.reshape(-1)\n",
    "\n",
    "regr.fit(X,y)\n",
    "r2 = regr.score(X,y)\n",
    "print(f'R2: {r2}')\n",
    "print(f'N. iteraciones: {regr.n_iter_}')\n",
    "y_pred = regr.predict(X)\n",
    "print(f'score: {r2_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar con el método partial_fit indicando el número de épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SGDRegressor(loss='squared_error', penalty=None, max_iter=100, learning_rate='constant', eta0=0.001, verbose=0)\n",
    "\n",
    "X = datos.iloc[:,:-1].values\n",
    "y = datos.iloc[:,-1].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y.reshape(-1,1))\n",
    "y = y.reshape(-1)\n",
    "\n",
    "n_epocas = 50\n",
    "for i in range(n_epocas):\n",
    "    regr.partial_fit(X,y)\n",
    "    r2 = regr.score(X,y)\n",
    "    print(f'R2: {r2}')\n",
    "    # print(f'Coeficientes: {regr.coef_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio de clase**\n",
    "\n",
    "Mostrar gráficamente el valor de $R^2$ en una gráfica. Probar valores para eta0 desde 1 hasta valores pequeños y observar que ocurre. Con el average se puede cambiar el comportamiento de stochastic a batch. Si average=True, los coeficientes se modifican al final de la época como el promedio del cambio de todos las correcciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SGDRegressor(loss='squared_error', penalty=None, max_iter=100, learning_rate='constant', eta0=0.1, verbose=0, average=False)\n",
    "\n",
    "X = datos.iloc[:,:-1].values\n",
    "y = datos.iloc[:,-1].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y.reshape(-1,1))\n",
    "y = y.reshape(-1)\n",
    "\n",
    "n_epocas = 100\n",
    "v = np.zeros(n_epocas)\n",
    "for i in range(n_epocas):\n",
    "    regr.partial_fit(X,y)\n",
    "    r2 = regr.score(X,y)\n",
    "    v[i] = r2\n",
    "\n",
    "plt.plot(v)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis dependencia lineal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar la bondad de los métodos de regresión lineal se puede utilizar el coeficiente de determinación $R^2$, que muestra la proporción del total de la variabilidad explicada por el modelo lineal. Esto se consigue comparando el resultado de utilizar el modelo lineal con una estimación basada en la media. Por tanto, cuanto más próxima sea la estimación al valor real y más alejada esté del valor medio, mejor será el modelo lineal obtenido. La expresión del coeficiente de determinación $R^2$ es la siguiente:\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^m(y_i -\\bar{y})^2}\n",
    "\\end{equation}\n",
    "donde $y_i$ e $\\hat{y}_i$ son los valores real y estimado, respectivamente, para la muestra i-ésima, y $\\bar{y}$ es el valor medio de la variable respuesta.\n",
    "\n",
    "De forma visual también se puede estimar la calidad del modelo de regresión lineal con respecto a una característica utilizando un diagrama de dispersión con la recta de regresión. Este diagrama es un gráfico bidimensional donde se muestran en forma de puntos los pares valor de la variable respuesta, valor de la característica, y de forma superpuesta la recta de regresión obtenida por el modelo. En la figura \\ref{fig:diagrama_dispersion} se puede observar un ejemplo de este tipo de diagramas donde se muestra la variable respuesta T\\_Final con respecto a la característica P1 del conjunto de datos utilizado en la práctica \\ref{ch:regresion}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio de clase**\n",
    "\n",
    "Dibujar la recta de regresión y los puntos para todas las variables mostrando el valor de $R^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('tiempos-minutos.csv', sep=';')\n",
    "X = datos.iloc[:,:-1].values\n",
    "y = datos.iloc[:,-1].values\n",
    "print(X.shape)\n",
    "variables = datos.columns\n",
    "regr = LinearRegression()\n",
    "for col in range(X.shape[1]):\n",
    "    X1 = X[:,col].reshape(-1,1)\n",
    "    # y_train1 = y_train\n",
    "\n",
    "    regr.fit(X1, y)\n",
    "\n",
    "    print(f'R2 para columna {variables[col]}: {r2:.2f}')\n",
    "\n",
    "    recta = regr.coef_ * X1 + regr.intercept_\n",
    "    plt.plot(X1, recta, 'r')\n",
    "    plt.scatter(X1, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordenar por relevancia\n",
    "\n",
    "Utilizar el coeficiente de determinación o el valor del peso asociado a cada variable para medir su importancia. De esta forma se puede ordenar las características en función de ese valor y no se necesario incluir todas en la estimación sino solo las más \"importantes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a la función de pérdida expresada en la ecuación (\\ref{eq:funcion_coste}), su minimización implica encontrar un conjunto de pesos de forma que la diferencia entre el valor estimado, $\\hat{y}_i$ y el valor real $y_i$ sea lo más pequeña posible. A priori existen muchos conjuntos de pesos que hacen mínima la función de pérdida, sin embargo se puede preferir conjuntos de pesos que posean una determinada característica frente a otros. Un caso puede ser cuando se busca un conjunto de pesos en los que no haya valores muy grandes para algunos de ellos de forma que se reduzcan proporcionalmente todos pero sin llegar a ser cero. Para ello, se introduce a la función de pérdida un término de regularización denominado $L2$, o regularización \\emph{Ridge},\n",
    "\n",
    "\\begin{equation}\n",
    "J_w = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j=0}^{n} w_i^2\n",
    "\\end{equation}\n",
    "\n",
    "En la función de pérdida (\\ref{eq:ridge}) el parámetro $\\lambda$ controla el efecto de la regularización. Para el caso de $\\lambda=0$ se tiene la función de perdida inicial, y a medida que se le da valores mayores, el efecto de la regularización es más fuerte.\n",
    "\n",
    "Otro caso opuesto al anterior, es cuando se desea que la estimación dependa de pocas variables haciendo que el peso de algunas de ellas sea muy bajo y de otras bastante alto. Esto se consigue con la regularización $L1$, o regularización \\emph{Lasso}, que añade a la función de perdida (\\ref{eq:funcion_coste}) un término que es la suma en valor absoluto de los coeficientes,\n",
    "\n",
    "\\begin{equation}\n",
    "J_w = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j=0}^{n} |w_i|\n",
    "\\end{equation}\n",
    "\n",
    "Al igual que en la regularización $L2$, el parámetro $\\lambda$ controla el efecto de la regularización. A medida que aumenta, los coeficientes tienen valores más dispersos pudiendo llegar en algunos casos a que determinados coeficientes sean casi cero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('tiempos-minutos.csv', sep=';')\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datos.iloc[:,:-1].values\n",
    "y = datos.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X,y)\n",
    "coef_lr = regr.coef_\n",
    "\n",
    "regr = Lasso(alpha=10)\n",
    "regr.fit(X,y)\n",
    "coef_lasso = regr.coef_\n",
    "\n",
    "regr = Ridge(alpha=10)\n",
    "regr.fit(X,y)\n",
    "coef_ridge = regr.coef_\n",
    "\n",
    "etiquetas = ['w'+str(i+1) for i in range(len(coef_lr))]\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.bar(etiquetas, coef_lr)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.bar(etiquetas, coef_lasso)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.bar(etiquetas, coef_ridge)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio de clase**\n",
    "\n",
    "Probar con diferentes valores de $lambda$ y mostrar el resultado gráficamente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PracticasAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
